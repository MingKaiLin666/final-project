{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [期末報告:預測外資持股比例]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 組員資料:\n",
    "統計四：劉明翰 105205018\n",
    "模型搭建、資料爬取\n",
    "\n",
    "應數碩二：林韋霖 107751006\n",
    "資料整理、資料爬取\n",
    "\n",
    "應數四：林銘凱 105701038 模型搭建、整合\n",
    "\n",
    "財政三：郭曜瑋 106205028\n",
    " 資料整理、資料爬取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 資料來源:\n",
    "https://www.stat.gov.tw/\n",
    "\n",
    "https://index.ndc.gov.tw/n/zh_tw\n",
    "\n",
    "https://www.twse.com.tw/zh/\n",
    "\n",
    "https://www.cnyes.com/\n",
    "\n",
    "https://www.cbc.gov.tw/tw/mp-1.html\n",
    "\n",
    "https://finance.yahoo.com/quote/%5EVIX/history/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 實做流程:\n",
    "1.抓取資料並依照日期及公司進行整合\n",
    "\n",
    "2.進行資料處理，建立資料集，每十五天為一單位，預測第16天的持股比例\n",
    "\n",
    "3.使用tcn模型進行預測\n",
    "\n",
    "https://medium.com/@cyeninesky3/%E6%99%82%E9%96%93%E5%8D%B7%E7%A9%8D%E7%B6%B2%E7%B5%A1-tcn-%E9%97%9C%E6%96%BC%E5%BE%9E%E9%A2%A8%E6%8E%A7%E9%A0%85%E7%9B%AE%E7%95%B6%E4%B8%AD%E7%9A%84%E5%AD%B8%E7%BF%92-11693d762f5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 資料前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_x(X):\n",
    "    X.columns = X.loc[0]\n",
    "\n",
    "    Delete_list_1 = [7, 8, 9, 10, 11, 12, 14, 16, 18, 20, 22, 24, 25, 27, 28, 30, 33, 36, 38, 40, 42]\n",
    "    Delete_list_2 = list(range(45, 57))\n",
    "    Delete_list_3 = [60, 61, 64, 65, 66, 68, 69]\n",
    "\n",
    "    D = Delete_list_1 + Delete_list_2 + Delete_list_3\n",
    "\n",
    "    ## remove index\n",
    "\n",
    "    for i in range(len(D)):\n",
    "        X = X.drop(D[i], axis=0)\n",
    "    ## remove comma\n",
    "\n",
    "    for i in range(1,16):\n",
    "        try:\n",
    "            X.loc[63][i] = X.loc[63][i].replace(\",\", \"\")\n",
    "        except:\n",
    "            print(f\"comma remove fail in index {i} \")\n",
    "\n",
    "    ## rescale list\n",
    "    list_rescale = [1, 2, 3, 4, 5, 6, 32, 57, 58, 63, 67]\n",
    "    list_n_rescale = [13, 15, 17, 19, 21, 23, 26, 29, 31, 34, 35, 37, 39, 41, 43, 59, 62]\n",
    "    list_i = [44]\n",
    "\n",
    "    ## replace to float\n",
    "    for i in list_rescale + list_n_rescale:\n",
    "        for j in range(1, 16):\n",
    "            try:\n",
    "                X.loc[i][j] = float(X.loc[i][j])\n",
    "            except:\n",
    "                print(f\"Fail to convert to float{i}\")\n",
    "\n",
    "    ## median normalization(TO DO: exponential transfer?)\n",
    "    all_issued_median = 458112177.99999994\n",
    "    MAD = 457654065.8219999\n",
    "    X.loc[63][1:] = (X.loc[63][1:] - all_issued_median)/(MAD)\n",
    "\n",
    "    list_rescale.remove(63)\n",
    "    ## rescale left\n",
    "    X_re = X.loc[list_rescale]\n",
    "\n",
    "    ## extract every first nonzero value in each row\n",
    "    norm = X_re.replace(0, np.nan).bfill(1).iloc[:, 1]\n",
    "\n",
    "    ## divide\n",
    "    X_re = X_re.iloc[:, 1:].divide(norm, axis=0)\n",
    "    X_re = X_re.fillna(0)\n",
    "\n",
    "\n",
    "    ## for non rescale part\n",
    "    X_n = X.loc[list_n_rescale]\n",
    "    X_n = X_n.replace(0, np.nan)\n",
    "    X_n = X_n.iloc[:, 1:] / 100\n",
    "    X_n.loc[59] = X_n.loc[59] - 1\n",
    "    X_n.loc[62] = X_n.loc[62] - 1\n",
    "    X_n = X_n.fillna(0)\n",
    "\n",
    "    ## for interest\n",
    "    X_i = X.loc[[44]].iloc[:, 1:]\n",
    "    X_i = X_i.fillna(0)\n",
    "\n",
    "    X = pd.concat([X_re,X_n,X_i],axis = 0)\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    X = pd.read_csv(\"train1-0.csv\")\n",
    "    Y = pd.read_csv(\"answer1-0.csv\")\n",
    "\n",
    "    df = preprocess_x(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/math/Desktop/TCN_foreign_invetsment-master/data_before/Xtrain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_L = os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ALL = initial\n",
    "for i in range(1,len(X_L)):\n",
    "    \n",
    "    X = pd.read_csv(X_L[i])\n",
    "    X = preprocess_X.preprocess_x(X)\n",
    "    X_train = np.transpose(X.values)\n",
    "    X_train = X_train.reshape(1,15,28)\n",
    "    X_train = np.asarray(X_train).astype(np.float32)\n",
    "    \n",
    "    X_ALL = np.concatenate([X_ALL,X_train])\n",
    "    \n",
    "    if i%100==0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/Users/math/Desktop/TCN_foreign_invetsment-master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('X_train.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"X_train\",  data=X_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('X_train.h5', 'r') as hf:\n",
    "    data = hf['X_train'][:]\n",
    "\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_y(Y):\n",
    "    Y_train = np.array([float(Y.loc[66][1])])\n",
    "    Y_train = Y_train/100\n",
    "    Y_train = Y_train.reshape(1,1)\n",
    "    return Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_ALL = initial\n",
    "for i in range(1,len(Y_L)):\n",
    "    Y = pd.read_csv(Y_L[i])\n",
    "    tmp = preprocess_y(Y)\n",
    "    Y_ALL = np.concatenate([Y_ALL,tmp])\n",
    "    \n",
    "    if i%100 == 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File('Y_train.h5', 'w') as hf:\n",
    "    hf.create_dataset(\"Y_train\",  data=Y_ALL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('Y_train.h5', 'r') as hf:\n",
    "    data = hf['Y_train'][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TCN模型\n",
    "\n",
    "This TCN model are based on the repo from https://github.com/philipperemy/keras-tcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import List\n",
    "\n",
    "from tensorflow.keras import backend as K, Model, Input, optimizers\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import Activation, SpatialDropout1D, Lambda ,Attention\n",
    "from tensorflow.keras.layers import Layer, Conv1D, Dense, BatchNormalization, LayerNormalization\n",
    "\n",
    "\n",
    "def is_power_of_two(num): ## 2^\n",
    "    return num != 0 and ((num & (num - 1)) == 0)\n",
    "\n",
    "\n",
    "def adjust_dilations(dilations): ## dilation rules\n",
    "    if all([is_power_of_two(i) for i in dilations]):\n",
    "        return dilations\n",
    "    else:\n",
    "        new_dilations = [2 ** i for i in dilations]\n",
    "        return new_dilations\n",
    "\n",
    "\n",
    "class ResidualBlock(Layer):\n",
    "\n",
    "    def __init__(self,\n",
    "                 dilation_rate,\n",
    "                 nb_filters,\n",
    "                 kernel_size,\n",
    "                 padding,\n",
    "                 activation='relu',\n",
    "                 dropout_rate=0,\n",
    "                 kernel_initializer='he_normal',\n",
    "                 use_batch_norm=False,\n",
    "                 use_layer_norm=False,\n",
    "                 **kwargs):\n",
    "\n",
    "        # type: (int, int, int, str, str, float, str, bool, bool, bool, dict) -> None\n",
    "        \"\"\"Defines the residual block for the WaveNet TCN\n",
    "\n",
    "        Args:\n",
    "            x: The previous layer in the model\n",
    "            training: boolean indicating whether the layer should behave in training mode or in inference mode\n",
    "            dilation_rate: The dilation power of 2 we are using for this residual block\n",
    "            nb_filters: The number of convolutional filters to use in this block\n",
    "            kernel_size: The size of the convolutional kernel\n",
    "            padding: The padding used in the convolutional layers, 'same' or 'causal'.\n",
    "            activation: The final activation used in o = Activation(x + F(x))\n",
    "            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "            kernel_initializer: Initializer for the kernel weights matrix (Conv1D).\n",
    "            use_batch_norm: Whether to use batch normalization in the residual layers or not.\n",
    "            use_layer_norm: Whether to use layer normalization in the residual layers or not.\n",
    "            kwargs: Any initializers for Layer class.\n",
    "        \"\"\"\n",
    "\n",
    "        self.dilation_rate = dilation_rate\n",
    "        self.nb_filters = nb_filters\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.activation = activation\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.layers = []\n",
    "        self.layers_outputs = []\n",
    "        self.shape_match_conv = None\n",
    "        self.res_output_shape = None\n",
    "        self.final_activation = None\n",
    "\n",
    "        super(ResidualBlock, self).__init__(**kwargs)\n",
    "\n",
    "    def _add_and_activate_layer(self, layer):\n",
    "        \"\"\"Helper function for building layer\n",
    "\n",
    "        Args:\n",
    "            layer: Appends layer to internal layer list and builds it based on the current output\n",
    "                   shape of ResidualBlocK. Updates current output shape.\n",
    "\n",
    "        \"\"\"\n",
    "        self.layers.append(layer)\n",
    "        self.layers[-1].build(self.res_output_shape)\n",
    "        self.res_output_shape = self.layers[-1].compute_output_shape(self.res_output_shape)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        with K.name_scope(self.name):  # name scope used to make sure weights get unique names\n",
    "            self.layers = []\n",
    "            self.res_output_shape = input_shape\n",
    "\n",
    "            for k in range(2):\n",
    "                name = 'conv1D_{}'.format(k)\n",
    "                with K.name_scope(name):  # name scope used to make sure weights get unique names\n",
    "                    self._add_and_activate_layer(Conv1D(filters=self.nb_filters,\n",
    "                                                        kernel_size=self.kernel_size,\n",
    "                                                        dilation_rate=self.dilation_rate,\n",
    "                                                        padding=self.padding,\n",
    "                                                        name=name,\n",
    "                                                        kernel_initializer=self.kernel_initializer))\n",
    "\n",
    "                with K.name_scope('norm_{}'.format(k)):\n",
    "                    if self.use_batch_norm:\n",
    "                        self._add_and_activate_layer(BatchNormalization())\n",
    "                    elif self.use_layer_norm:\n",
    "                        self._add_and_activate_layer(LayerNormalization())\n",
    "\n",
    "                self._add_and_activate_layer(Activation('relu'))\n",
    "                self._add_and_activate_layer(SpatialDropout1D(rate=self.dropout_rate))\n",
    "\n",
    "            if self.nb_filters != input_shape[-1]:\n",
    "                # 1x1 conv to match the shapes (channel dimension).\n",
    "                name = 'matching_conv1D'\n",
    "                with K.name_scope(name):\n",
    "                    # make and build this layer separately because it directly uses input_shape\n",
    "                    self.shape_match_conv = Conv1D(filters=self.nb_filters,\n",
    "                                                   kernel_size=1,\n",
    "                                                   padding='same',\n",
    "                                                   name=name,\n",
    "                                                   kernel_initializer=self.kernel_initializer)\n",
    "\n",
    "            else:\n",
    "                name = 'matching_identity'\n",
    "                self.shape_match_conv = Lambda(lambda x: x, name=name)\n",
    "\n",
    "            with K.name_scope(name):\n",
    "                self.shape_match_conv.build(input_shape)\n",
    "                self.res_output_shape = self.shape_match_conv.compute_output_shape(input_shape)\n",
    "\n",
    "            self.final_activation = Activation(self.activation)\n",
    "            self.final_activation.build(self.res_output_shape)  # probably isn't necessary\n",
    "\n",
    "            # this is done to force Keras to add the layers in the list to self._layers\n",
    "            for layer in self.layers:\n",
    "                self.__setattr__(layer.name, layer)\n",
    "            self.__setattr__(self.shape_match_conv.name, self.shape_match_conv)\n",
    "            self.__setattr__(self.final_activation.name, self.final_activation)\n",
    "\n",
    "            super(ResidualBlock, self).build(input_shape)  # done to make sure self.built is set True\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        \"\"\"\n",
    "        Returns: A tuple where the first element is the residual model tensor, and the second\n",
    "                 is the skip connection tensor.\n",
    "        \"\"\"\n",
    "        x = inputs\n",
    "        self.layers_outputs = [x]\n",
    "        for layer in self.layers:\n",
    "            training_flag = 'training' in dict(inspect.signature(layer.call).parameters)\n",
    "            x = layer(x, training=training) if training_flag else layer(x)\n",
    "            self.layers_outputs.append(x)\n",
    "        x2 = self.shape_match_conv(inputs)\n",
    "        self.layers_outputs.append(x2)\n",
    "        res_x = layers.add([x2, x])\n",
    "        self.layers_outputs.append(res_x)\n",
    "\n",
    "        res_act_x = self.final_activation(res_x)\n",
    "        self.layers_outputs.append(res_act_x)\n",
    "        return [res_act_x, x]\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return [self.res_output_shape, self.res_output_shape]\n",
    "\n",
    "\n",
    "class TCN(Layer):\n",
    "    \"\"\"Creates a TCN layer.\n",
    "\n",
    "        Input shape:\n",
    "            A tensor of shape (batch_size, timesteps, input_dim).\n",
    "\n",
    "        Args:\n",
    "            nb_filters: The number of filters to use in the convolutional layers.\n",
    "            kernel_size: The size of the kernel to use in each convolutional layer.\n",
    "            dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
    "            nb_stacks : The number of stacks of residual blocks to use.\n",
    "            padding: The padding to use in the convolutional layers, 'causal' or 'same'.\n",
    "            use_skip_connections: Boolean. If we want to add skip connections from input to each residual blocK.\n",
    "            return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "            activation: The activation used in the residual blocks o = Activation(x + F(x)).\n",
    "            dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "            kernel_initializer: Initializer for the kernel weights matrix (Conv1D).\n",
    "            use_batch_norm: Whether to use batch normalization in the residual layers or not.\n",
    "            kwargs: Any other arguments for configuring parent class Layer. For example \"name=str\", Name of the model.\n",
    "                    Use unique names when using multiple TCN.\n",
    "\n",
    "        Returns:\n",
    "            A TCN layer.\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 nb_filters=64,\n",
    "                 kernel_size=2,\n",
    "                 nb_stacks=1,\n",
    "                 dilations=(1, 2, 4, 8, 16, 32),\n",
    "                 padding='causal',\n",
    "                 use_skip_connections=False,\n",
    "                 dropout_rate=0.0,\n",
    "                 return_sequences=False,\n",
    "                 activation='relu',\n",
    "                 kernel_initializer='he_normal',\n",
    "                 use_batch_norm=False,\n",
    "                 use_layer_norm=False,\n",
    "                 **kwargs):\n",
    "\n",
    "        self.return_sequences = return_sequences\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.use_skip_connections = use_skip_connections\n",
    "        self.dilations = dilations\n",
    "        self.nb_stacks = nb_stacks\n",
    "        self.kernel_size = kernel_size\n",
    "        self.nb_filters = nb_filters\n",
    "        self.activation = activation\n",
    "        self.padding = padding\n",
    "        self.kernel_initializer = kernel_initializer\n",
    "        self.use_batch_norm = use_batch_norm\n",
    "        self.use_layer_norm = use_layer_norm\n",
    "        self.skip_connections = []\n",
    "        self.residual_blocks = []\n",
    "        self.layers_outputs = []\n",
    "        self.build_output_shape = None\n",
    "        self.lambda_layer = None\n",
    "        self.lambda_ouput_shape = None\n",
    "\n",
    "        if padding != 'causal' and padding != 'same':\n",
    "            raise ValueError(\"Only 'causal' or 'same' padding are compatible for this layer.\")\n",
    "\n",
    "        if not isinstance(nb_filters, int):\n",
    "            print('An interface change occurred after the version 2.1.2.')\n",
    "            print('Before: tcn.TCN(x, return_sequences=False, ...)')\n",
    "            print('Now should be: tcn.TCN(return_sequences=False, ...)(x)')\n",
    "            print('The alternative is to downgrade to 2.1.2 (pip install keras-tcn==2.1.2).')\n",
    "            raise Exception()\n",
    "\n",
    "        # initialize parent class\n",
    "        super(TCN, self).__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def receptive_field(self):\n",
    "        assert_msg = 'The receptive field formula works only with power of two dilations.'\n",
    "        assert all([is_power_of_two(i) for i in self.dilations]), assert_msg\n",
    "        return self.kernel_size * self.nb_stacks * self.dilations[-1]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "\n",
    "        # member to hold current output shape of the layer for building purposes\n",
    "        self.build_output_shape = input_shape\n",
    "\n",
    "        # list to hold all the member ResidualBlocks\n",
    "        self.residual_blocks = []\n",
    "        total_num_blocks = self.nb_stacks * len(self.dilations)\n",
    "        if not self.use_skip_connections:\n",
    "            total_num_blocks += 1  # cheap way to do a false case for below\n",
    "\n",
    "        for s in range(self.nb_stacks):\n",
    "            for d in self.dilations:\n",
    "                self.residual_blocks.append(ResidualBlock(dilation_rate=d,\n",
    "                                                          nb_filters=self.nb_filters,\n",
    "                                                          kernel_size=self.kernel_size,\n",
    "                                                          padding=self.padding,\n",
    "                                                          activation=self.activation,\n",
    "                                                          dropout_rate=self.dropout_rate,\n",
    "                                                          use_batch_norm=self.use_batch_norm,\n",
    "                                                          use_layer_norm=self.use_layer_norm,\n",
    "                                                          kernel_initializer=self.kernel_initializer,\n",
    "                                                          name='residual_block_{}'.format(len(self.residual_blocks))))\n",
    "                # build newest residual block\n",
    "                self.residual_blocks[-1].build(self.build_output_shape)\n",
    "                self.build_output_shape = self.residual_blocks[-1].res_output_shape\n",
    "\n",
    "        # this is done to force keras to add the layers in the list to self._layers\n",
    "        for layer in self.residual_blocks:\n",
    "            self.__setattr__(layer.name, layer)\n",
    "\n",
    "        # Author: @karolbadowski.\n",
    "        output_slice_index = int(self.build_output_shape.as_list()[1] / 2) if self.padding == 'same' else -1\n",
    "        self.lambda_layer = Lambda(lambda tt: tt[:, output_slice_index, :])\n",
    "        self.lambda_ouput_shape = self.lambda_layer.compute_output_shape(self.build_output_shape)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\"\n",
    "        Overridden in case keras uses it somewhere... no idea. Just trying to avoid future errors.\n",
    "        \"\"\"\n",
    "        if not self.built:\n",
    "            self.build(input_shape)\n",
    "        if not self.return_sequences:\n",
    "            return self.lambda_ouput_shape\n",
    "        else:\n",
    "            return self.build_output_shape\n",
    "\n",
    "    def call(self, inputs, training=None):\n",
    "        x = inputs\n",
    "        self.layers_outputs = [x]\n",
    "        self.skip_connections = []\n",
    "        for layer in self.residual_blocks:\n",
    "            x, skip_out = layer(x, training=training)\n",
    "            self.skip_connections.append(skip_out)\n",
    "            self.layers_outputs.append(x)\n",
    "\n",
    "        if self.use_skip_connections:\n",
    "            x = layers.add(self.skip_connections)\n",
    "            self.layers_outputs.append(x)\n",
    "\n",
    "        if not self.return_sequences:\n",
    "            x = self.lambda_layer(x)\n",
    "            self.layers_outputs.append(x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        \"\"\"\n",
    "        Returns the config of a the layer. This is used for saving and loading from a model\n",
    "        :return: python dictionary with specs to rebuild layer\n",
    "        \"\"\"\n",
    "        config = super(TCN, self).get_config()\n",
    "        config['nb_filters'] = self.nb_filters\n",
    "        config['kernel_size'] = self.kernel_size\n",
    "        config['nb_stacks'] = self.nb_stacks\n",
    "        config['dilations'] = self.dilations\n",
    "        config['padding'] = self.padding\n",
    "        config['use_skip_connections'] = self.use_skip_connections\n",
    "        config['dropout_rate'] = self.dropout_rate\n",
    "        config['return_sequences'] = self.return_sequences\n",
    "        config['activation'] = self.activation\n",
    "        config['use_batch_norm'] = self.use_batch_norm\n",
    "        config['use_layer_norm'] = self.use_layer_norm\n",
    "        config['kernel_initializer'] = self.kernel_initializer\n",
    "        return config\n",
    "\n",
    "\n",
    "def compiled_tcn(num_feat,  # type: int\n",
    "                 num_classes,  # type: int\n",
    "                 nb_filters,  # type: int\n",
    "                 kernel_size,  # type: int\n",
    "                 dilations,  # type: List[int]\n",
    "                 nb_stacks,  # type: int\n",
    "                 max_len,  # type: int\n",
    "                 output_len=1,  # type: int\n",
    "                 padding='causal',  # type: str\n",
    "                 use_skip_connections=False,  # type: bool\n",
    "                 return_sequences=True,\n",
    "                 regression=False,  # type: bool\n",
    "                 dropout_rate=0.05,  # type: float\n",
    "                 name='tcn',  # type: str,\n",
    "                 kernel_initializer='he_normal',  # type: str,\n",
    "                 activation='relu',  # type:str,\n",
    "                 opt='adam',\n",
    "                 lr=0.002,\n",
    "                 use_batch_norm=False,\n",
    "                 use_layer_norm=False):\n",
    "    # type: (...) -> Model\n",
    "    \"\"\"Creates a compiled TCN model for a given task (i.e. regression or classification).\n",
    "    Classification uses a sparse categorical loss. Please input class ids and not one-hot encodings.\n",
    "\n",
    "    Args:\n",
    "        num_feat: The number of features of your input, i.e. the last dimension of: (batch_size, timesteps, input_dim).\n",
    "        num_classes: The size of the final dense layer, how many classes we are predicting.\n",
    "        nb_filters: The number of filters to use in the convolutional layers.\n",
    "        kernel_size: The size of the kernel to use in each convolutional layer.\n",
    "        dilations: The list of the dilations. Example is: [1, 2, 4, 8, 16, 32, 64].\n",
    "        nb_stacks : The number of stacks of residual blocks to use.\n",
    "        max_len: The maximum sequence length, use None if the sequence length is dynamic.\n",
    "        padding: The padding to use in the convolutional layers.\n",
    "        use_skip_connections: Boolean. If we want to add skip connections from input to each residual blocK.\n",
    "        return_sequences: Boolean. Whether to return the last output in the output sequence, or the full sequence.\n",
    "        regression: Whether the output should be continuous or discrete.\n",
    "        dropout_rate: Float between 0 and 1. Fraction of the input units to drop.\n",
    "        activation: The activation used in the residual blocks o = Activation(x + F(x)).\n",
    "        name: Name of the model. Useful when having multiple TCN.\n",
    "        kernel_initializer: Initializer for the kernel weights matrix (Conv1D).\n",
    "        opt: Optimizer name.\n",
    "        lr: Learning rate.\n",
    "        use_batch_norm: Whether to use batch normalization in the residual layers or not.\n",
    "        use_layer_norm: Whether to use layer normalization in the residual layers or not.\n",
    "    Returns:\n",
    "        A compiled keras TCN.\n",
    "    \"\"\"\n",
    "\n",
    "    dilations = adjust_dilations(dilations)\n",
    "\n",
    "    input_layer = Input(shape=(max_len, num_feat))\n",
    "\n",
    "    x = TCN(nb_filters, kernel_size, nb_stacks, dilations, padding,\n",
    "            use_skip_connections, dropout_rate, return_sequences,\n",
    "            activation, kernel_initializer, use_batch_norm, use_layer_norm,\n",
    "            name=name)(input_layer)\n",
    "\n",
    "    print('x.shape=', x.shape)\n",
    "\n",
    "    def get_opt():\n",
    "        if opt == 'adam':\n",
    "            return optimizers.Adam(lr=lr, clipnorm=1.)\n",
    "        elif opt == 'rmsprop':\n",
    "            return optimizers.RMSprop(lr=lr, clipnorm=1.)\n",
    "        else:\n",
    "            raise Exception('Only Adam and RMSProp are available here')\n",
    "\n",
    "    if not regression:\n",
    "        # classification\n",
    "        x = Dense(num_classes)(x)\n",
    "        x = Activation('softmax')(x)\n",
    "        output_layer = x\n",
    "        model = Model(input_layer, output_layer)\n",
    "\n",
    "        # https://github.com/keras-team/keras/pull/11373\n",
    "        # It's now in Keras@master but still not available with pip.\n",
    "        # TODO remove later.\n",
    "        def accuracy(y_true, y_pred):\n",
    "            # reshape in case it's in shape (num_samples, 1) instead of (num_samples,)\n",
    "            if K.ndim(y_true) == K.ndim(y_pred):\n",
    "                y_true = K.squeeze(y_true, -1)\n",
    "            # convert dense predictions to labels\n",
    "            y_pred_labels = K.argmax(y_pred, axis=-1)\n",
    "            y_pred_labels = K.cast(y_pred_labels, K.floatx())\n",
    "            return K.cast(K.equal(y_true, y_pred_labels), K.floatx())\n",
    "\n",
    "        model.compile(get_opt(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    else:\n",
    "        # regression\n",
    "        #x=Attention()([x,x])\n",
    "        x = Dense(output_len)(x)\n",
    "        x = Activation('sigmoid')(x) ##change to sigmoid\n",
    "        output_layer = x\n",
    "        model = Model(input_layer, output_layer)\n",
    "        model.compile(get_opt(), loss='mean_squared_error', metrics=['accuracy'])\n",
    "    print('model.x = {}'.format(input_layer.shape))\n",
    "    print('model.y = {}'.format(output_layer.shape))\n",
    "    return model\n",
    "\n",
    "\n",
    "def tcn_full_summary(model, expand_residual_blocks=True):\n",
    "    layers = model._layers.copy()  # store existing layers\n",
    "    model._layers.clear()  # clear layers\n",
    "\n",
    "    for i in range(len(layers)):\n",
    "        if isinstance(layers[i], TCN):\n",
    "            for layer in layers[i]._layers:\n",
    "                if not isinstance(layer, ResidualBlock):\n",
    "                    if not hasattr(layer, '__iter__'):\n",
    "                        model._layers.append(layer)\n",
    "                else:\n",
    "                    if expand_residual_blocks:\n",
    "                        for lyr in layer._layers:\n",
    "                            if not hasattr(lyr, '__iter__'):\n",
    "                                model._layers.append(lyr)\n",
    "                    else:\n",
    "                        model._layers.append(layer)\n",
    "        else:\n",
    "            model._layers.append(layers[i])\n",
    "\n",
    "    model.summary()  # print summary\n",
    "\n",
    "    # restore original layers\n",
    "    model._layers.clear()\n",
    "    [model._layers.append(lyr) for lyr in layers]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tcn import compiled_tcn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File('X_train.h5', 'r') as hf:\n",
    "    X = hf['X_train'][:]\n",
    "\n",
    "with h5py.File('Y_train.h5', 'r') as hf:\n",
    "    Y = hf['Y_train'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77526, 15, 28)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[np.isnan(X)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77526, 1)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=87)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54268, 15, 28)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[np.isnan(X_train)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(54268, 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape= (None, 24)\n",
      "model.x = (None, 15, 28)\n",
      "model.y = (None, 1)\n"
     ]
    }
   ],
   "source": [
    "model = compiled_tcn(return_sequences=False,\n",
    "                             num_feat=28,\n",
    "                             nb_filters=24,\n",
    "                             num_classes=0,\n",
    "                             kernel_size=8,\n",
    "                             dilations=[2 ** i for i in range(9)],\n",
    "                             nb_stacks=1,\n",
    "                             max_len=15,\n",
    "                             use_skip_connections=True,\n",
    "                             regression=True,\n",
    "                             dropout_rate=0,\n",
    "                             output_len=1) ##opt adam with lr=0.002 default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 15, 28)]          0         \n",
      "_________________________________________________________________\n",
      "tcn (TCN)                    (None, 24)                84840     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 25        \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 84,865\n",
      "Trainable params: 84,865\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1086/1086 [==============================] - 152s 140ms/step - loss: 0.0277\n",
      "Epoch 2/20\n",
      "1086/1086 [==============================] - 158s 146ms/step - loss: 0.0239\n",
      "Epoch 3/20\n",
      "1086/1086 [==============================] - 152s 140ms/step - loss: 0.0220\n",
      "Epoch 4/20\n",
      "1086/1086 [==============================] - 155s 142ms/step - loss: 0.0208\n",
      "Epoch 5/20\n",
      "1086/1086 [==============================] - 152s 140ms/step - loss: 0.0203\n",
      "Epoch 6/20\n",
      "1086/1086 [==============================] - 159s 147ms/step - loss: 0.0199\n",
      "Epoch 7/20\n",
      "1086/1086 [==============================] - 159s 146ms/step - loss: 0.0198\n",
      "Epoch 8/20\n",
      "1086/1086 [==============================] - 157s 145ms/step - loss: 0.0196\n",
      "Epoch 9/20\n",
      "1086/1086 [==============================] - 156s 144ms/step - loss: 0.0195\n",
      "Epoch 10/20\n",
      "1086/1086 [==============================] - 157s 144ms/step - loss: 0.0193\n",
      "Epoch 11/20\n",
      "1086/1086 [==============================] - 155s 142ms/step - loss: 0.0193\n",
      "Epoch 12/20\n",
      "1086/1086 [==============================] - 157s 144ms/step - loss: 0.0193\n",
      "Epoch 13/20\n",
      "1086/1086 [==============================] - 159s 146ms/step - loss: 0.0192\n",
      "Epoch 14/20\n",
      "1086/1086 [==============================] - 157s 144ms/step - loss: 0.0190\n",
      "Epoch 15/20\n",
      "1086/1086 [==============================] - 161s 148ms/step - loss: 0.0190\n",
      "Epoch 16/20\n",
      "1086/1086 [==============================] - 162s 149ms/step - loss: 0.0188\n",
      "Epoch 17/20\n",
      "1086/1086 [==============================] - 152s 140ms/step - loss: 0.0188\n",
      "Epoch 18/20\n",
      "1086/1086 [==============================] - 155s 143ms/step - loss: 0.0187\n",
      "Epoch 19/20\n",
      "1086/1086 [==============================] - 167s 154ms/step - loss: 0.0187\n",
      "Epoch 20/20\n",
      "1086/1086 [==============================] - 159s 146ms/step - loss: 0.0186\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a6f7f1810>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=50, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "543/543 [==============================] - 160s 295ms/step - loss: 0.0182\n",
      "Epoch 2/10\n",
      "543/543 [==============================] - 161s 297ms/step - loss: 0.0181\n",
      "Epoch 3/10\n",
      "543/543 [==============================] - 172s 316ms/step - loss: 0.0181\n",
      "Epoch 4/10\n",
      "543/543 [==============================] - 161s 297ms/step - loss: 0.0182\n",
      "Epoch 5/10\n",
      "543/543 [==============================] - 176s 324ms/step - loss: 0.0181\n",
      "Epoch 6/10\n",
      "543/543 [==============================] - 173s 319ms/step - loss: 0.0181\n",
      "Epoch 7/10\n",
      "543/543 [==============================] - 161s 297ms/step - loss: 0.0180\n",
      "Epoch 8/10\n",
      "543/543 [==============================] - 167s 307ms/step - loss: 0.0180\n",
      "Epoch 9/10\n",
      "543/543 [==============================] - 158s 290ms/step - loss: 0.0180\n",
      "Epoch 10/10\n",
      "543/543 [==============================] - 158s 291ms/step - loss: 0.0180\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a4c148510>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=100, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "272/272 [==============================] - 165s 606ms/step - loss: 0.0176\n",
      "Epoch 2/5\n",
      "272/272 [==============================] - 155s 570ms/step - loss: 0.0176\n",
      "Epoch 3/5\n",
      "272/272 [==============================] - 155s 571ms/step - loss: 0.0176\n",
      "Epoch 4/5\n",
      "272/272 [==============================] - 165s 608ms/step - loss: 0.0175\n",
      "Epoch 5/5\n",
      "272/272 [==============================] - 165s 607ms/step - loss: 0.0175\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a4c092f50>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=200, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "272/272 [==============================] - 163s 599ms/step - loss: 0.0176\n",
      "Epoch 2/10\n",
      "272/272 [==============================] - 163s 598ms/step - loss: 0.0175\n",
      "Epoch 3/10\n",
      "272/272 [==============================] - 165s 608ms/step - loss: 0.0175\n",
      "Epoch 4/10\n",
      "272/272 [==============================] - 164s 602ms/step - loss: 0.0174\n",
      "Epoch 5/10\n",
      "272/272 [==============================] - 161s 592ms/step - loss: 0.0175\n",
      "Epoch 6/10\n",
      "272/272 [==============================] - 159s 583ms/step - loss: 0.0175\n",
      "Epoch 7/10\n",
      "272/272 [==============================] - 159s 584ms/step - loss: 0.0174\n",
      "Epoch 8/10\n",
      "272/272 [==============================] - 166s 612ms/step - loss: 0.0174\n",
      "Epoch 9/10\n",
      "272/272 [==============================] - 161s 593ms/step - loss: 0.0174\n",
      "Epoch 10/10\n",
      "272/272 [==============================] - 170s 624ms/step - loss: 0.0173\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a4c4bc710>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=200, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "543/543 [==============================] - 164s 302ms/step - loss: 0.0177\n",
      "Epoch 2/10\n",
      "543/543 [==============================] - 164s 301ms/step - loss: 0.0177\n",
      "Epoch 3/10\n",
      "543/543 [==============================] - 161s 296ms/step - loss: 0.0176\n",
      "Epoch 4/10\n",
      "543/543 [==============================] - 169s 311ms/step - loss: 0.0176\n",
      "Epoch 5/10\n",
      "543/543 [==============================] - 162s 298ms/step - loss: 0.0176\n",
      "Epoch 6/10\n",
      "543/543 [==============================] - 165s 303ms/step - loss: 0.0174\n",
      "Epoch 7/10\n",
      "543/543 [==============================] - 163s 301ms/step - loss: 0.0175\n",
      "Epoch 8/10\n",
      "543/543 [==============================] - 162s 298ms/step - loss: 0.0173\n",
      "Epoch 9/10\n",
      "543/543 [==============================] - 164s 302ms/step - loss: 0.0174\n",
      "Epoch 10/10\n",
      "543/543 [==============================] - 166s 306ms/step - loss: 0.0174\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a4affce10>"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size=100, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "727/727 [==============================] - 18s 25ms/step - loss: 0.0177\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.017671450972557068"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /Users/math/Desktop/TCN_foreign_invetsment-master/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"/Users/math/Desktop/TCN_foreign_invetsment-master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
